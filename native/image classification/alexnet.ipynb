{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/benjin.zhu/.libs/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(num_classes, dtype='float32', **kwargs):\n",
    "    input_data = mx.sym.Variable(name='data')\n",
    "    if dtype == 'float16':\n",
    "        input_data = mx.sym.Cast(input_data, dtype=np.float16)\n",
    "    # Stage 1\n",
    "    conv1 = mx.sym.Convolution(name='conv1', data=input_data, kernel=(11, 11), stride=(4, 4), num_filter=96)\n",
    "    relu1 = mx.sym.Activation(data=conv1, act_type='relu')\n",
    "    lrn1 = mx.sym.LRN(data=relu1, alpha=0.0001, beta=0.75, knorm=2, nsize=5)\n",
    "    pool1 = mx.sym.Pooling(data=lrn1, pool_type='max', kernel=(3, 3), stride=(2, 2))\n",
    "    # Stage 2\n",
    "    conv2 = mx.sym.Convolution(name='conv2', data=pool1, kernel=(5, 5), pad=(3, 3), num_filter=256)\n",
    "    relu2 = mx.sym.Activation(data=conv2, act_type='relu')\n",
    "    lrn2 = mx.sym.LRN(data=relu2, alpha=0.0001, beta=0.75, knorm=2, nsize=5)\n",
    "    pool2 = mx.sym.Pooling(data=lrn2, pool_type='max', kernel=(3, 3), stride=(2, 2))\n",
    "    # Stage 3\n",
    "    conv3 = mx.sym.Convolution(name='conv3', data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=384)\n",
    "    relu3 = mx.sym.Activation(data=conv3, act_type=\"relu\")\n",
    "    conv4 = mx.sym.Convolution(name='conv4', data=relu3, kernel=(3, 3), pad=(1, 1), num_filter=384)\n",
    "    relu4 = mx.sym.Activation(data=conv4, act_type=\"relu\")\n",
    "    conv5 = mx.sym.Convolution(name='conv5', data=relu4, kernel=(3, 3), pad=(1, 1), num_filter=256)\n",
    "    relu5 = mx.sym.Activation(data=conv5, act_type=\"relu\")\n",
    "    pool3 = mx.sym.Pooling(data=relu5, kernel=(3, 3), stride=(2, 2), pool_type='max')\n",
    "    # Stage 4\n",
    "    flatten = mx.sym.Flatten(pool3)\n",
    "    fc1 = mx.sym.FullyConnected(data=flatten, name='fc1', num_hidden=4096)\n",
    "    relu6 = mx.sym.Activation(data=fc1, act_type='relu')\n",
    "    dropout1 = mx.sym.Dropout(data=relu6, p=0.5)\n",
    "    # Stage 5\n",
    "    fc2 = mx.sym.FullyConnected(name='fc2', data=dropout1, num_hidden=4096)\n",
    "    relu7 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "    dropout2 = mx.sym.Dropout(data=relu7, p=0.5)\n",
    "    # Stage 6\n",
    "    fc3 = mx.sym.FullyConnected(data=dropout2, name='fc3', num_hidden=num_classes)\n",
    "    if dtype == 'float16':\n",
    "        fc3 = mx.sym.Cast(fc3, dtype=np.float32)\n",
    "    smo = mx.sym.SoftmaxOutput(data=fc3, name='softmax')\n",
    "    \n",
    "    return smo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
