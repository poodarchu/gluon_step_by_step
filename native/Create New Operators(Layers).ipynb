{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorials walks you through the process of creating new MXNet operators(or layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poodar/Library/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Custom Op\n",
    "import os\n",
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(mx.operator.CustomOp):\n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        x = in_data[0].asnumpy()\n",
    "        y = np.exp(x-x.max(axis=1).reshape((x.shape[0], 1)))\n",
    "        y /= y.sum(axis=1).reshape((x.shape[0], 1))\n",
    "        # At the end, we used CustomOp.assign to assign the resulting array y to out_data[0]. \n",
    "        # It handles assignment based on the value of req, which can be ‘write’, ‘add’, or ‘null’.\n",
    "        self.assign(out_data[0], req[0], mx.nd.array(y))\n",
    "        \n",
    "    def backard(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        l = in_data[1].asnumpy().ravel().astype(np.int)\n",
    "        y = out_data[0].asnumpy()\n",
    "        y[np.arange(l.shape[0]), l] -= 1.0\n",
    "        self.assign(in_grad[0], req[0], mx.nd.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to define its input/output format by subclassing mx.operator.CustomOpProp. \n",
    "# First, register the new operator with the name ‘softmax’:\n",
    "@mx.operator.register(\"softmax\")\n",
    "class SoftmaxProp(mx.operator.CustomOpProp):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxProp, self).__init__(need_top_grad=False)\n",
    "        \n",
    "    def list_arguments(self):\n",
    "        return ['data', 'label']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "    \n",
    "    # provide infer_shape to declare the shape of the output/weight \n",
    "    # and check the consistency of the input shapes\n",
    "    def infer_shape(self, in_shape):\n",
    "        data_shape = in_shape[0]\n",
    "        label_shape = (in_shape[0][0],)\n",
    "        output_shape = in_shape[0]\n",
    "        # The infer_shape function should always return three lists in this order: \n",
    "        # inputs, outputs, and auxiliary states \n",
    "        return [data_shape, label_shape], [output_shape], []\n",
    "    \n",
    "    def infer_type(self, in_type):\n",
    "        dtype = in_type[0]\n",
    "        return [dtype, dtype], [dtype], []\n",
    "\n",
    "    # Finally define a create_operator function that will be calle by the back end to create an instance of softmax:\n",
    "    def create_operator(self, ctx, shapes, dtypes):\n",
    "        return Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Custom operator:\n",
    "# define mlp\n",
    "data = mx.symbol.Variable('data')\n",
    "\n",
    "fc1 = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)\n",
    "act1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)\n",
    "act2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\n",
    "fc3 = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)\n",
    "#mlp = mx.symbol.Softmax(data = fc3, name = 'softmax')\n",
    "mlp = mx.symbol.Custom(data=fc3, name='softmax1', op_type='softmax') # op_type 对应前面　ｒｅｇｉｓｔｅｒ　的　ｏｐ　名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_mnist_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d31f49c6b894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# MXNET_CPU_WORKER_NTHREADS must be greater than 1 for custom op to work on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_mnist_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "train, val = get_mnist_iterator(batch_size=100, input_shape=(784, ))\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# MXNET_CPU_WORKER_NTHREADS must be greater than 1 for custom op to work on CPU\n",
    "context = mx.cpu()\n",
    "\n",
    "mod = mx.mod.Module(mlp, context=context)\n",
    "\n",
    "mod.fit(train_data=train, \n",
    "        eval_data=val, \n",
    "        optimizer='sgd', \n",
    "        optimizer_params={'learning_rate':0.1, 'momentum':0.9, 'wd':0.00001},\n",
    "        num_epoch=10, \n",
    "        batch_end_callback=mx.callback.Speedometer(100, 100)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
